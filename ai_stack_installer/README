ArgusIoT service stack installer script.

Currently script and this instruction is tested on:
 -Vagrant VM
 -AWS VM
 -General server VMs (Win, ESXi) with Ubuntu.

It works reliably to install - Hbase, OpenTSDB & Grafana.

To stand a service stack on a VM involves 5 high level steps:
1. Creating an installer_bundle
2. Copying the installer bundle to your VM
3. Logging in to the VM
4. Using the installer bundle inside the VM to create a new service stack
5. Verifying the stack is running, accepting data, and serving web UIs.

Dockerized Stack Instructions:
==============================

These are the steps required to stand up a stack using Docker as a "VM"
Kiran wrote these steps on 5/16/22; they should be executed from this directory

Generate the service stack installer bundle:
 
 make installer_bundle
 ls -l output/
 # expect to find this file: ai_stack_installer.tar.gz 

Create the Docker containers for OpenTSDB and Grafana, and connect them
via network bridge:
 
 make init_stack

This will build and run the Docker images, eventually resulting in a
bash interface within the OpenTSDB container

If you want to shut down the stack and remove all containers/images,
type the following command in the bash terminal:

 exit

Then you will be returned to your OS terminal, where you will type
the following:

 make close_stack

And the entire stack will be purged from Docker and system memory
Do note that the installer bundle generated from the earlier
make command will persist until its make command is invoked again

To continue with the initialization of the stack,
open a new terminal window and type the following:
 
 docker network inspect argus
 # expect to see 2 objects in the "Containers" field, one named
 # "grafana" and one named "opentsdb"

Take note of the IP Address assigned to the container named "opentsdb"
ending before the slash character, for example we might see

 "Name": "opentsdb",
 "EndpointID": "662b1adaa32b8168dbff7e46feef1811cb5cc214e401239268be0347cd94b453",
 "MacAddress": "02:42:ac:19:00:03",
 "IPv4Address": "172.25.0.3/16",
 "IPv6Address": ""

And what we want to capture is "172.25.0.3"
When we eventually want to point Grafana to our OpenTSDB container, 
we give it that same IP address appended with ":4242"
For example, this is what we would type into Grafana

 172.25.0.3:4242

To inspect OpenTSDB or Grafana, point your browser to the following URLs:

 http://localhost:4242 to manage OpenTSDB
 http://localhost:3000 to manage Grafana

If you want to see the stack working in action, you can run a test file
by returning to the bash terminal within the OpenTSDB container
and typing the following:

 python3 http_post_using_json_simple_script.py

That's it! For the purposes of working with Docker, you can ignore the rest of
this README.


Detailed steps:
===============

Create the installer bundle
------------------------------------------------
Clone the repos into the dir on the host where you plan to have your dev
work root:

 mkdir argusiot; cd argusiot
 git clone https://github.com/argusiot/data_platform
 git clone https://github.com/argusiot/argus_exporter
 # Save this dir. We'll use it below...
 WRKDIR=`pwd`

Generate the service stack installer bundle:

 cd $WRKDIR/data_platform/ai_stack_installer
 make installer_bundle
 ls -l output/
 # expect to find this file: ai_stack_installer.tar.gz 


For vagrant VM only:
------------------------
If you're using Vagrant for VMs, the argus_exporter repo already has a ready
made Vagrantfile, that you can use, for creating a Vagrant VM from the host
where the repo is checked out.
This Vagrantfile creates a Ubuntu VM with /argusiot dir as a shared directory
with the host. /argusiot on the VM maps to the parent dir of the dir where
the vagrant VM was started.

Start in the dev work dir on your host (same we created above):

 cd $WRKDIR
 cd argus_exporter
 vagrant up

Connect to the vagrant VM to see that it comes up:

 vagrant ssh

If all checks out exit back to your host

 exit



Copy the installer bundle to your VM:
----------------------------------------

The way to copy the installer to the VM differs slightly between the
different VMs, due to how they are created.

AWS example:
------------

scp -i ~/.ssh/GeneralPurposeTestAWSVM.pem $WRKDIR/data_platform/ai_stack_installer/output/ai_stack_installer.tar.gz ubuntu@34.218.224.94:~/.

AWS will use a private key that should already be copied to your ~/.ssh dir.
The AWS user is always 'ubuntu'.
The IP address is for the AWS instance you are installing on.

General server example:
(e.g. Ubuntu server VM on ESXi on Dell server blade)
----------------------------------------------------

a) with private key:

 scp -i ~/.ssh/myvmkey $WRKDIR/data_platform/ai_stack_installer/output/ai_stack_installer.tar.gz tsadmin@192.168.1.154:~/.

b) with password

 scp $WRKDIR/data_platform/ai_stack_installer/output/ai_stack_installer.tar.gz tsadmin@192.168.1.154:~/.

The server user can be any admin user you choose. We've commonly used 'tsadmin'.
The IP address is for the server you are installing on.

Vagrant VM example:
--------------------

cd $WRKDIR/argus_exporter
vagrant upload $WRKDIR/data_platform/ai_stack_installer/output/ai_stack_installer.tar.gz 

Vagrant supports "vagrant upload" as an alternative to scp without any
additional setup, or need to know the VM IP adress.
If destination path is not given it uploads to /home/vagrant by default.
Note: don't give ~/. as destination path here as ~ will translate to your
home dir on the host instead of on the Vagrant VM.


Login to your VM
-----------------

AWS example:
------------

ssh -i ~/.ssh/GeneralPurposeTestAWSVM.pem ubuntu@34.218.224.94


General server example:
------------------------

a) with private key:

 ssh -i ~/.ssh/myvmkey tsadmin@192.168.1.154

b) with password

 ssh tsadmin@192.168.1.154


Vagrant VM example:
--------------------

cd $WRKDIR/argus_exporter
vagrant ssh

Vagrant supports "vagrant ssh" as an alternative to ssh without any
additional setup, or need to know the VM IP adress.


Install the stack:
------------------
(These commands are run from the login shell on the VM)

cd ~/
mkdir stack_install
mv ai_stack_installer.tar.gz stack_install/.
cd stack_install
tar -xvf ai_stack_installer.tar.gz

sudo ./argusiot_service_stack_install.sh


Verification:
--------------
See if the needed ports are up:
netstat -l | grep 4242 
netstat -l | grep 3000
netstat -l | grep 2181

4242 - OpenTSDB
3000 - Grafana
2181 - ZooKeeper

telnet localhost 2181
stat
Expect to see Zookeeper version

Capabilities now enabled:
-------------------------
 -- OpenTSDB R/W will now work
 -- Accessing Grafana will work.


Adding data to OpenTSDB:
------------------------

In the stack_install directory there is a test tool for adding data to the
TSDB. It "simulates" a slow remote client psuhing one data point at a time.
Run it on the VM using:

cd ~/stack_install
python3 http_post_using_json_simple_script.py

If you have data in a specifcally formatted csv file, you can also use the
virtual exporter and importer in argus_exporter to add data to the TSDB,
see the README for the virtual exporter in the $WRKDIR on your host computer
for eloquent ways to do this:

$WRKDIR/argus_exporter/iot_gw/virtual_exporter/README.md

Finding ip address of the VM:
-----------------------------

The IP address for your AWS and server VMs are same as the ones you used for
ssh in the copy and login steps.

The vagrant VM is initialized with a private network address that can be reached from the host. Find the private network address (should be 172.x.x.x):

ifconfig enp0s8


Accessing OpenTSDB viewer:
--------------------------

Open a web browser on your host and open (if VM ip is 172.28.128.3)

http://172.28.128.3:4242

(Note: this may not work on AWS server, unless you access from a remote node
that is on the same VPN as the VM)

Setting up Grafana:
-------------------

Open a web browser on your host and open (if VM ip is 172.28.128.3)

http://172.28.128.3:3000

Login with the default admin credentials (admin/admin).
Provide new password. No need to make this complicated on a local server VM,
like a Vagrant VM, or internal server blade, since the VM may be destroyed at
any time)
If you want to add another user, do so as editor, but it is not needed on a VM, since the VM may be destroyed at any time).
(These steps are important when standing up the stack on cloud server, like AWS)


Config the local OpenTSBD as a new data source, following this instruction:

https://grafana.com/docs/grafana/latest/datasources/add-a-data-source/

Choose OpenTSDB and explicitly type "http://localhost:4242".
Don't leave the default.
You'll know where to type it when you see it.

Select Version ==2.3
Select Resolution=second for WCPL and millisecond for CC.

Click [SAVE and TEST]

Now Grafana is ready to use, and as soon as you have added any data to OpenTSDB you can visualize it by creating dashboards or just "Exploring" the data.


Restarting services after restarting VM:
----------------------------------------

If you halt or restart your VM, OpenTSDB and ZooKeeper need to be restarted after the VM comes up. Grafana willl restart automatically.

These are the steps:

# Log in to youru VM.
# Start OpenTSDB and ZooKeeper
cd /usr/share/hbase/hbase-1.4.14/
sudo bin/start-hbase.sh
sudo jps
# Verify HMaster is running. Can take a few seconds.
sudo service opentsdb restart

Note:
The hbase directory name may vary depending on what hbase version got installed.
You can get the correct name by using shell completion (TAB-key) to get the right one, instead of copy-pasting the command from above.


# Optional DB inspection steps:
    ./bin/hbase shell
    => list
       # (should see 4 tables: tsdb, tsdb-meta, tsdb-tree, tsdb-uid)
    => exit

# Optional : zookeeper verification
    telnet localhost 2181
    stat  # (should show: â€œZookeeper version: 3.4.10 . . . . <more stuff>]

To stop the OpenTSDB service (e.g. to restart with above steps):

sudo service opentsdb stop
cd /usr/share/hbase/hbase-1.4.13/
sudo bin/stop-hbase.sh
sudo jps
